# AI Tool Stack Voice Note

> The user is discussing their AI tool stack and the progress they've made with it, including components like Ollama, ASR, whisper, and the challenge of local AI with an AMD GPU. They cover topics like dockerization, microservices, API integration, and their plans to create a podcast using AI.

*Transcribed: 3 Dec 2025 22:05*

---

All right, so this is the uh first voice note uh in today's using some voice notes. I'm just really ranting off my tool stack at the end of the year and I'm making uh very, very amazing progress with Claude on all these components, such that I feel like in a couple of days, I'll kind of have all these things I really wanted to have for the last year. It's kind of like I guess the the geeky equivalent to Christmas come early and getting all my goodies for uh the next year and beyond. So this uh particular component is um here is here's my motivation for what I asked for and I'm very, very impressed with the control panel that has been generated so far. It's actually kind of it's really nice. It's it's a beautiful little UI. And it's actually kind of exactly what I was looking for. Um so I have an AMD GPU, which I kind of got before I became before I got into AI stuff. And it was great, um until I said, oh, wait, I can do some local AI stuff or my GPU is, you know, good enough at 12 gigabytes VRAM. It's a Radeon 7700 that I can probably play around with some of this local AI stuff. And uh then I sort of began hitting all these walls related to AMD and Rockm and stuff really only supporting Nvidia. Um in general, I have a hard time wrapping my head around Python environments and dependence and there's so many different package managers. And uh turning to a couple of friends who work with Python and said like, how do you guys just I guess I'm sort of um old school in the sense that when I build these huge images, Rockm and Pytorch, I keep thinking, I can't keep creating these, I'm going to run out of space. Um, I grew up in an area where the modem squeaked to download four megabytes. So the idea of repetitively pulling 20 and 30 gigabyte packages kind of makes me squeamish, um, or nervous. I don't know, that my ISP is going to cut off my internet. I have no idea what I'm worried about, but at any event, it seems kind of inefficient. So I stumbled upon conda for kind of working around this, like saying I have a couple of big environments, Pytorch Rockm, and then I build out for the specific workload. And, um, nothing really against conda except that it seemed uh just a lot of complications involved with it and kind of felt like I was using something the wrong thing. So, reluctantly, I decided, let's just try to dockerize everything. Um, I've nothing against Docker. In fact, I think Docker is amazing, but I didn't really understand, I think that one of the advantages of Docker is this layering mechanism of uh, like I understood the isolation benefits, but not I didn't actually realize there was utility there in caching and reusing layers in different stacks. And when I realized that, I'm like, wait, Docker actually solves exactly what I was trying to solve with conda. So, why don't I just use Docker? I already know kind of how to use it. Um, I guess it seemed the disadvantage was isolation from the from the host. That seemed that seemed like that always, you know, I I worried that it's going to be super complicated to get pass through and stuff like that. Anyway, I'm I'm just giving this context in case over time the idea pulls back towards um maybe being mixed conda Docker, but and just just explain how we kind of got to this point of dockerizing all these stacks. So, for local AI, basically, um the services we have now, there's Ollama. And if I'm not mistaken, that's in Docker. If it's not in Docker, because I think I have it on host as well, then we should I guess uh uninstall on host and move move over the um the models because I hate again from my scarcity mentality with uh digital electronics, I hate um pulling and repulling these gigantic models. And I I think it's better to have one set up or the other. Um so the ones I use. Yeah, Ollama is brilliant for um I use it mostly, I rarely, rarely use it for anything like chat. I don't really use chat interface locally, but I do use it for scripting and especially for kind of text. But more than that, I think it's got utility in sitting alongside the other stack components here. ASR and Whisper is actually what brought me to make this because it's been um challenging to I've been seeing there's a lot of whisper tech out there, but frequently the Rockm issue is a big deal. So, sort of what I figured out with voice detects and ASR was just have that backbone, Pytorch Rockm, solid, don't use bleeding edge, keep it kind of like reliable. Sit back from the cutting edge a little bit there. And then you can just kind of layer stuff into it. That's the my kind of ideal um mechanism. So, the purpose of this repository was actually to to kind of uh firstly, I had a lot of stuff in in Conda still and caches and wasting a ton of space and I wanted to have a single source of truth for these are the local AI services. It's all being brought up from here. It's a Docker deployment. And after doing that, then I was like, well, the only other just the only other reason I don't like using Docker is simply UI. Um if I start something one of the other things that took me a while to figure out with Docker is like I always had this idea that running anything involved like a lot of resource usage and then I saw that actually idle containers don't really use much if at all. Uh so having a way that the even if they're auto starting, being able to stop them, being able to change that without needing to go into portainer and the Docker compose and editing manually. And that's something that if that can be done, um visually is a lot more easy and friendly than trying to remember where's it deploying from. Um etcetera. So, for the stack components, we actually have, I'm just trying to make sure I've got this right. I think we're only lacking I think we're only lacking one. So, the idea in this server was basically inference. Um it's not these aren't finished products or these these are more like it's kind of what I'd like to have really is something that's a unified stack. It starts on boost and I know that the stuff I need is there um without needing to individually start up each Docker component and that they're not they're not they're not going to run into port conflicts. They're spaced out etcetera. So the other um idea I think I noted is here was that I wanted to there's one particular workflow that I think really works well together, which is whisper in Ollama and for speech to text. Um, I'm trying to use MCP in a way that maybe there's maybe at this microservice level, um, this is it. Like the so the the the answer to how do I create services that um use them both is just you create the services that use them both, you have them both available. You run stuff through Whisper and then you run it through Ollama. And I think that's probably the answer. Um, but in my planning notes, I was kind of thinking more along the lines of should we define different stacks for like should we have Whisper Ollama and then a Whisper plus Ollama stack. Um, looking at the stack and guessing there's no need to actually do that. So, the the only missing component now is uh TTS. Um, so for TTS, the main utility for me would be creating um creating podcasts. I'm just looking through the repository because I was certain I recorded this earlier today. Apparently not. Okay, so for TTS, um, I'm working on a using a workflow I've tried to use a workflow for uh creating podcasts with AI. So typically it's using two different uh voices and something like single shot cloning. They're like character voices. And generally the the system that I've kind of created is I have my prompt, it an agent creates a script. Script gets read out, diarized by the two hosts and that then gets concatenated with my prompt to give me the episode. So that's its own project because I really like to use it frequently. But um, where this local uh inference server comes into play is that it would be useful to to have the um everything ready for that to happen. So there's a couple that um I'm just noting and looking at what I've noted here for TTS models, Vib voice, Dia, DIA, Kakoro and Fish speech. Um future ASR options, Whisper, faster whisper, finetuned models. So an MCP server. So I think really the question is where do I want to draw the line between the stuff I want to pack into this uh AI server and the stuff that I want to implement at the program level and that's probably the direction that I kind of would tend towards is try to keep this just the core services running and then the specificities handled um by the applications. But the other little microservices that would be useful here. Um, for Whisper X, which I if I'm not mistaken is is diarizing Whisper with diarization, um, comes in handy for this podcast workflow a lot. Nice to have if it doesn't add a lot of background weight as a process. Um, and VAD right now we cannot, but let's just I'm just recording for again for just as a note uh that VAD is a very is a useful component in uh speech detects really. And I'm conscious, I don't know if I've already reinvented a wheel here, but I try not to do that. Um, so there's like bigger projects for TTS, web UI that I'm just trying to say, I don't need all those bells and whistles. I just want to like, so but if there is even anything like a a stack that can be incorporated into this just for the TTS, the baseline functionality I'd like to have is just a human sounding, natural sounding TTS voice. Um and some of the models, Kakoro Dia, the only question is if they can run on AMD. And I think it's more a question of rather than looking at the differences, what can run the most easily. Um, voice cloning is not really, I would say a key requirement in the sense that yes, for my podcast, it makes I really like to use my character voices. But if there's just a few stock voices, I can work with that. Right now, just be nice to have something that is part of the stack that's brought up and I know that I can use it for TTS. So that's here part two of the sort of plan for this project that just kind of came to me as I was looking at this. Because we talked about MCP and um part of what I'm doing at the moment is creating uh MCP servers for all the stuff that I've been doing manually repetitively with AI that's absolutely amazing that it can work. Stuff like moving data to my NAS. Um to have sort of a local MCP server that has all those tools ready. So I can just connect it and never again have to repeat the same information about my NAS, the LAN address, the where the volumes are, it's an ology. That's a tool that I can just have to repeat. And for this local AI inference server, it would be tremen the main use for this is actually all local AI um for local agents to use. So, for example, in Claude code, uh I'm creating an an MCP for local transcription. And that's why I want this the transcription API to be accessible and running so that I can say it's here and not have to that that MCP server doesn't have to deal with the additional complication of creating the stack, the stack is there, it's on the environment. Um, it'd be really bummer to find out that something already existed for this. Uh I'm sure probably for Nvidia. I figured for AMD and my specific utilities, it might just be a specific enough to do um to do like this. So, for MCP uh model context protocol, we're wrapping around APIs. That's the fundamental technology. So, so long as these services provide local APIs, we're kind of good. And an open API compatible API is kind of the key, it's machine readable, great, very easy then to to scaffold in MCP. So, my initial thought was, okay, let's add TTS and then let's make sure that they're all providing a local API and that it's there's uh each one has an API definition and then somewhere in this uh control panel, um I can have links to the definitions and one maybe master definition. That was idea number one. Idea number two was something I would have regarded a short while ago as way too complicated and now I'm thinking maybe this actually does make sense, which is rather than have four parallel APIs, local APIs, one for Ollama, one for speech detects, one for Whisper, one for Comfy UI, we'll have one local AI API that has the open API schema and it's proxying between to all these backend services. And I think that's probably actually the that would be a pretty slick architecture to strive towards and I think probably what we should uh whether it's a incremental progress to get there or not feasible. I'll just put it down as a note for how I think this would be really cool.
